{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Transformers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In scikit-learn, you can create custom transformers using either a class-based approach or a function-based approach. Each method has its advantages, and the choice often depends on your specific use case and preferences. Here’s a detailed comparison of both approaches:\n",
    "\n",
    "### Class-Based Approach\n",
    "\n",
    "**Overview:**\n",
    "The class-based approach involves creating a custom transformer by defining a class that implements the required methods (usually `fit`, `transform`, and optionally `fit_transform`). This is the recommended method for building reusable and maintainable transformers.\n",
    "\n",
    "**Advantages:**\n",
    "- **Encapsulation:** You can encapsulate related functionality and state (e.g., parameters, learned statistics) in a single class.\n",
    "- **Scikit-learn Integration:** Classes can easily be integrated into scikit-learn pipelines, allowing for consistent behavior across different transformers.\n",
    "- **Reusability:** Once defined, you can create multiple instances of the class with different parameters.\n",
    "\n",
    "\n",
    "### Function-Based Approach\n",
    "\n",
    "**Overview:**\n",
    "The function-based approach involves defining a standalone function that takes data as input and returns the transformed data. This approach is simpler and can be quicker to implement for one-off transformations or when you don’t need to maintain state.\n",
    "\n",
    "**Advantages:**\n",
    "- **Simplicity:** It’s easier and quicker to write for simple transformations.\n",
    "- **Less Overhead:** No need to define a class structure, which can be beneficial for simple tasks.\n",
    "\n",
    "\n",
    "### Comparison\n",
    "\n",
    "| Aspect                    | Class-Based Approach                          | Function-Based Approach                       |\n",
    "|---------------------------|----------------------------------------------|----------------------------------------------|\n",
    "| **Structure**             | Encapsulated in a class with methods         | Simple function                                |\n",
    "| **State Maintenance**     | Can maintain state (e.g., learned parameters) | No state is maintained                        |\n",
    "| **Reusability**           | Easily reusable with different instances      | Reusable but typically for simpler tasks     |\n",
    "| **Integration with Pipelines** | Directly integrates with scikit-learn pipelines | Requires additional wrappers for integration  |\n",
    "| **Complexity**            | More complex, suitable for advanced use cases | Simpler, suitable for quick transformations   |\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "- Use the **class-based approach** if you need a reusable, maintainable transformer that integrates seamlessly with scikit-learn’s functionality and requires state management.\n",
    "- Use the **function-based approach** for simple, one-off transformations that don’t require the overhead of a class structure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Example OF class Approach* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class Robust_Scaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    A robust scaler that scales features based on the interquartile range (IQR) \n",
    "    to make the model robust to outliers. This transformer scales data by removing \n",
    "    the median and dividing by the IQR (75th percentile - 25th percentile), making \n",
    "    it less sensitive to outliers compared to standard scaling techniques.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    iqr : array-like, shape (n_features,)\n",
    "        The interquartile range (IQR) for each feature, calculated as the \n",
    "        difference between the 75th and 25th percentile of the data.\n",
    "    \n",
    "    medians : array-like, shape (n_features,)\n",
    "        The median for each feature, calculated during the fitting process.\n",
    "\n",
    "    Methods\n",
    "    -------\n",
    "    fit(X, y=None)\n",
    "        Computes the IQR and median for each feature in the dataset `X`.\n",
    "    \n",
    "    transform(X)\n",
    "        Scales the features in the dataset `X` using the IQR and median values.\n",
    "        Raises an error if the scaler has not been fitted yet.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.iqr = None\n",
    "        self.medians = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Compute the interquartile range (IQR) and median for each feature \n",
    "        in the dataset `X`.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input data to compute the statistics.\n",
    "        \n",
    "        y : Ignored\n",
    "            Not used, present here for consistency with scikit-learn API.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns the instance of the transformer.\n",
    "        \"\"\"\n",
    "        self.iqr = np.percentile(X, 75, axis=0) - np.percentile(X, 25, axis=0)\n",
    "        self.medians = np.median(X, axis=0)\n",
    "\n",
    "        # Handle division by zero for features with constant values\n",
    "        self.iqr[self.iqr == 0] = 1\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"\n",
    "        Scale the input data `X` using the IQR and medians calculated during fitting.\n",
    "        The transformation is applied to each feature individually.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            The input data to be transformed.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        X_scaled : array-like, shape (n_samples, n_features)\n",
    "            The transformed data with features scaled by the IQR.\n",
    "        \n",
    "        Raises\n",
    "        ------\n",
    "        RuntimeError\n",
    "            If the `fit` method has not been called before `transform`.\n",
    "        \"\"\"\n",
    "        # Ensure that fit has been called\n",
    "        if self.medians is None or self.iqr is None:\n",
    "            raise RuntimeError(\"Missing medians or IQR values. Please fit the transformer first.\")\n",
    "        \n",
    "        # Apply IQR scaling to each feature\n",
    "        return (X - self.medians) / self.iqr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *The formula of above class*\n",
    "Robust scaling is used when the data contains outliers. It scales the data based on the median and the interquartile range (IQR), making it less sensitive to extreme values.\n",
    "\n",
    "The formula for Robust Scaling is:\n",
    "\n",
    "![Min-Max Scaling Formula](Robust%20Scaling.png)\n",
    "\n",
    "```\n",
    "X_scaled = (X - median(X)) / IQR\n",
    "```\n",
    "\n",
    "- `median(X)` is the median of the feature values.\n",
    "- `IQR` is the interquartile range (difference between the 75th and 25th percentiles)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Example Usage of the Custom Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic data\n",
    "\n",
    "X, _ = make_blobs(n_samples=100, n_features=2, centers=3, random_state=42)\n",
    "\n",
    "# Create and fit the transformer\n",
    "custom_tra = Robust_Scaler()\n",
    "fit_mo = custom_tra.fit(X)\n",
    "\n",
    "# Transform the data\n",
    "X_transformed = fit_mo.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.60302683,  1.92268979])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_tra.medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple = (X[0] - custom_tra.medians) / custom_tra.iqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.49872679, -0.71613207])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.72642091, -8.39495682])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.49872679, -0.71613207])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_transformed[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Example OF Function Approach*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_scaler(X, scale_factor=1):\n",
    "    return X * scale_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "custom_tra = FunctionTransformer(custom_scaler, kw_args={'scale_factor': 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "# Generate synthetic data\n",
    "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
    "\n",
    "X_transformed = custom_tra.transform(X)\n",
    "\n",
    "# Scale the transformed data\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "fit_mo = lin_reg.fit(X_transformed, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
